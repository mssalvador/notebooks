{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KaptialDataFrame.parquet', 'virkdata.parquet', 'AllApsAs.parquet', 'KvartalsVaerker.parquet', 'AarsVaerker.parquet', 'MaanedsVaerker.parquet', 'TotalAarsVaerker.parquet']\n"
     ]
    }
   ],
   "source": [
    "#this notebook generates Kapitals for cvrData\n",
    "#Always Pyspark first!\n",
    "ErhvervsPath = \"/home/svanhmic/workspace/Python/Erhvervs\"\n",
    "\n",
    "from pyspark.sql import functions as F, Window, WindowSpec\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StringType,ArrayType,IntegerType,DoubleType,StructField,StructType\n",
    "sc.addPyFile(ErhvervsPath+\"/src/RegnSkabData/ImportRegnskabData.py\")\n",
    "sc.addPyFile(ErhvervsPath+'/src/RegnSkabData/RegnskabsClass.py')\n",
    "sc.addPyFile(ErhvervsPath+'/src/cvr/GetNextJsonLayer.py')\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import ImportRegnskabData\n",
    "import GetNextJsonLayer\n",
    "import itertools\n",
    "import functools\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "\n",
    "cvrPath = \"/home/svanhmic/workspace/Python/Erhvervs/data/cdata/parquet\"\n",
    "cvrfiles = os.listdir(cvrPath)\n",
    "print(cvrfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import crv data\n",
    "cvrDf = (sqlContext\n",
    "         .read\n",
    "         .parquet(cvrPath+\"/\"+cvrfiles[1])\n",
    "        )\n",
    "\n",
    "#Extract all Aps and A/S companies\n",
    "virkformCols = (\"cvrNummer\",\"virksomhedsform\")\n",
    "\n",
    "virkformDf = GetNextJsonLayer.createNextLayerTable(cvrDf.select(*virkformCols),[virkformCols[0]],virkformCols[1])\n",
    "virkformDf = GetNextJsonLayer.expandSubCols(virkformDf,mainColumn=\"periode\")\n",
    "virkformDf = (virkformDf\n",
    "              .drop(\"sidstOpdateret\")\n",
    "              .withColumn(col=F.col(\"periode_gyldigFra\").cast(\"date\"),colName=\"periode_gyldigFra\")\n",
    "              .withColumn(col=F.col(\"periode_gyldigTil\").cast(\"date\"),colName=\"periode_gyldigTil\")\n",
    "             )\n",
    "\n",
    "#virkformDf.show(1)\n",
    "checkCols = [\"kortBeskrivelse\",\"langBeskrivelse\",\"virksomhedsformkode\"]\n",
    "\n",
    "#Consistencycheck is kortBeskrivelse and virksomhedsformkode always mapped the same way\n",
    "#check1 = virkformDf.select(checkCols+[\"cvrNummer\"]).distinct().groupby(*checkCols).count()\n",
    "#check1.orderBy(\"kortBeskrivelse\",\"count\").show(check1.count(),truncate=False)\n",
    "\n",
    "#Second test does any companies go from Aps or A/S to other or vice versa?\n",
    "joinCols = [\"cvrNummer\",\"langBeskrivelse\",\"rank\"]\n",
    "cvrCols = [\"cvrNummer\"]\n",
    "gyldigCol = [\"periode_gyldigFra\"]\n",
    "\n",
    "statusChangeWindow = (Window\n",
    "                      .partitionBy(F.col(*cvrCols))\n",
    "                      .orderBy(F.col(\"periode_gyldigFra\").desc()))\n",
    "\n",
    "#virkformDf.select(checkCols).distinct().show(50,truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "#Extract APS and AS here and latest status...\n",
    "aggregationCols = [F.max(i) for i in gyldigCol]\n",
    "groupsCol = [i for i in virkformDf.columns if i not in gyldigCol]\n",
    "\n",
    "companyByAsApsDf = (virkformDf\n",
    "                    .where((F.col(\"virksomhedsformkode\") == 60) | (F.col(\"virksomhedsformkode\") == 80))\n",
    "                    .withColumn(col=F.rank().over(statusChangeWindow),colName=\"rank\")\n",
    "                    .filter(F.col(\"rank\") == 1)\n",
    "                    .cache()\n",
    "                   )\n",
    "\n",
    "companyByAsApsDf.write.parquet(cvrPath+\"/AllApsAs.parquet\",mode=\"overwrite\")\n",
    "#companyByAsApsDf.printSchema()\n",
    "#compDf = companyByAsApsDf.groupBy(*companyByAsApsDf.columns).agg(F.last(F.col(\"rank\")).over(rankWindow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pivotOnText(df,**kvargs):\n",
    "    '''\n",
    "        does the pivotation on text cols and removes the excess counts\n",
    "        input df - dataframe \n",
    "        kvargs - optional arguments included are: \n",
    "                pivotCol - specify column that shoould be pivotated, default type\n",
    "                valueCol - specify column that should be aggregated on, defalut vaerdi\n",
    "                expectedList - specify the values in the pivotated column, default [\"KAPITAL\"]\n",
    "    '''\n",
    "    \n",
    "    #sets some of the optional parameters\n",
    "    pivotCol = kvargs.get(\"pivotCol\",\"type\")\n",
    "    expectedList = kvargs.get(\"expectedList\",[\"KAPITAL\"])\n",
    "    valueCol = kvargs.get(\"valueCol\",\"vaerdi\")\n",
    "    \n",
    "    holdOutsCols = [pivotCol,valueCol]\n",
    "    nonHoldOutCols = [i for i in df.columns if i not in holdOutsCols]\n",
    "\n",
    "    \n",
    "    newDf = (df\n",
    "             .groupBy(df.columns)\n",
    "             .count()\n",
    "             .groupBy(*nonHoldOutCols)\n",
    "             .pivot(pivotCol,expectedList)\n",
    "             .agg(F.max(F.struct(\"count\",valueCol)))\n",
    "           )\n",
    "    expandedDf = GetNextJsonLayer.expandSubCols(newDf,*expectedList)\n",
    "    newCols = [i for i in expandedDf.columns if i not in [v+\"_count\" for v in expectedList] ]\n",
    "    return expandedDf.select(newCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get Attributes data frame\n",
    "attributDf = GetNextJsonLayer.createNextLayerTable(cvrDf,[\"cvrNummer\"],\"attributter\")\n",
    "orderedAttattributDf = attributDf.groupBy(\"type\").count().orderBy(F.col(\"count\").desc())\n",
    "attributDf.registerTempTable(\"attribut\")\n",
    "extractedList = (\"KAPITAL\",\"KAPITALVALUTA\",\"KAPITALKLASSER\",\"KAPITAL_DELVIST\")\n",
    "\n",
    "#use sql commands to use filter\n",
    "sqlExprs = \"SELECT * FROM attribut WHERE \"\n",
    "for i in extractedList:\n",
    "    sqlExprs += ' type == \"'+i+ '\" OR'\n",
    "    \n",
    "filtAttributDf = sqlContext.sql(sqlExprs[0:-3])\n",
    "\n",
    "vaerdi = \"vaerdier\"\n",
    "filterdCols = [ i for i in filtAttributDf.columns if i not in (vaerdi)]\n",
    "\n",
    "filtAttributDf = GetNextJsonLayer.createNextLayerTable(filtAttributDf,filterdCols,vaerdi)\n",
    "filtAttributDf = GetNextJsonLayer.expandSubCols(filtAttributDf,\"periode\")\n",
    "filtAttributDf.registerTempTable(\"filteredAttributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------+----------+--------------------+------------+-----------------+-----------------+\n",
      "|cvrNummer|sekvensnr|         type|vaerditype|      sidstOpdateret|      vaerdi|periode_gyldigFra|periode_gyldigTil|\n",
      "+---------+---------+-------------+----------+--------------------+------------+-----------------+-----------------+\n",
      "| 10005019|        0|      KAPITAL|   decimal|2015-02-10T00:00:...|-13929317.00|       1999-11-04|       2000-01-18|\n",
      "| 10005019|        0|      KAPITAL|   decimal|2015-02-10T00:00:...| -4729317.00|       2000-01-19|       2000-04-17|\n",
      "| 10005019|        0|      KAPITAL|   decimal|2015-02-10T00:00:...|  -153840.00|       2000-04-18|       2001-03-29|\n",
      "| 10005019|        0|      KAPITAL|   decimal|2015-02-10T00:00:...|         .00|       2001-03-30|       2002-10-29|\n",
      "| 10005019|        0|      KAPITAL|   decimal|2015-02-10T00:00:...|    67300.00|       2002-10-30|       2003-01-07|\n",
      "| 10005019|        1|      KAPITAL|   decimal|2015-02-10T00:00:...|-14121617.00|       2003-01-08|       2003-01-08|\n",
      "| 10005019|        0|      KAPITAL|   decimal|2015-02-10T00:00:...|-14054317.00|       2003-01-08|       2013-05-31|\n",
      "| 10005019|        0|KAPITALVALUTA|    string|2015-02-10T00:00:...|         DKK|       1999-11-04|       2000-01-18|\n",
      "| 10005019|        0|KAPITALVALUTA|    string|2015-02-10T00:00:...|         DKK|       2000-01-19|       2000-04-17|\n",
      "| 10005019|        0|KAPITALVALUTA|    string|2015-02-10T00:00:...|         DKK|       2000-04-18|       2001-03-29|\n",
      "| 10005019|        0|KAPITALVALUTA|    string|2015-02-10T00:00:...|         DKK|       2001-03-30|       2002-10-29|\n",
      "| 10005019|        0|KAPITALVALUTA|    string|2015-02-10T00:00:...|         EUR|       2002-10-30|       2003-01-07|\n",
      "| 10005019|        0|KAPITALVALUTA|    string|2015-02-10T00:00:...|         EUR|       2003-01-08|       2013-05-31|\n",
      "| 10005019|        1|KAPITALVALUTA|    string|2015-02-10T00:00:...|         DKK|       2003-01-08|       2003-01-08|\n",
      "| 10130123|        1|      KAPITAL|   decimal|2015-02-09T21:00:...|    16813.73|       2003-01-31|       2003-01-31|\n",
      "| 10130123|        0|      KAPITAL|   decimal|2015-02-09T21:00:...|   125000.00|       2003-01-31|       2003-11-03|\n",
      "| 10130123|        0|      KAPITAL|   decimal|2015-02-09T21:00:...|    83186.27|       2003-11-04|       2005-05-11|\n",
      "| 10130123|        0|      KAPITAL|   decimal|2015-02-09T21:00:...|    98186.27|       2005-05-12|       2006-12-19|\n",
      "| 10130123|        0|      KAPITAL|   decimal|2015-02-09T21:00:...|    98187.27|       2006-12-20|       2013-12-04|\n",
      "| 10130123|        1|KAPITALVALUTA|    string|2015-02-09T21:00:...|         EUR|       2003-01-31|       2003-01-31|\n",
      "+---------+---------+-------------+----------+--------------------+------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "dataType should be DataType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8f86c0583721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0myearMonDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaaned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munix_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"01\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"yyyy-mm-dd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1997\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    316\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    317\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"not supported type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, dataType, nullable, metadata)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataType should be DataType\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"field name should be string\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: dataType should be DataType"
     ]
    }
   ],
   "source": [
    "orderedCols = [F.col(i) for i in (\"cvrNummer\",\"type\",\"periode_gyldigFra\")]\n",
    "#filtAttributDf.orderBy(*orderedCols).show()\n",
    "\n",
    "#take a look at the entries with longer sequence numbers \n",
    "(sqlContext\n",
    " .sql(\"SELECT * FROM filteredAttributes AS l WHERE EXISTS (SELECT DISTINCT cvrNummer FROM filteredAttributes AS r WHERE sekvensnr >= 1 AND l.cvrNummer = r.cvrNummer)\")\n",
    " .orderBy(*orderedCols)\n",
    " .show()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---------+\n",
      "| aar|maaned|       ts|\n",
      "+----+------+---------+\n",
      "|1997|     1|852073200|\n",
      "|1997|     2|854751600|\n",
      "|1997|     3|857170800|\n",
      "|1997|     4|859845600|\n",
      "|1997|     5|862437600|\n",
      "|1997|     6|865116000|\n",
      "|1997|     7|867708000|\n",
      "|1997|     8|870386400|\n",
      "|1997|     9|873064800|\n",
      "|1997|    10|875656800|\n",
      "|1997|    11|878338800|\n",
      "|1997|    12|880930800|\n",
      "|1998|     1|883609200|\n",
      "|1998|     2|886287600|\n",
      "|1998|     3|888706800|\n",
      "|1998|     4|891381600|\n",
      "|1998|     5|893973600|\n",
      "|1998|     6|896652000|\n",
      "|1998|     7|899244000|\n",
      "|1998|     8|901922400|\n",
      "+----+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yearMonDf = (sqlContext\n",
    "             .createDataFrame([Row(aar=i,maaned=j,ts=str(i)+\"-\"+str(j)+\"-1\") for i in range(1997,2017) for j in range(1,13)])\n",
    "             .withColumn(col=F.unix_timestamp(F.col(\"ts\").cast(\"date\")),colName=\"ts\")\n",
    "            )\n",
    "yearMonDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+--------------+--------------------+---------------------+----------------------+--------+------------+------------+\n",
      "|cvrNummer| gyldigFra| gyldigTil|KAPITAL_vaerdi|KAPITALVALUTA_vaerdi|KAPITALKLASSER_vaerdi|KAPITAL_DELVIST_vaerdi|datediff|timeStampFra|timeStampTil|\n",
      "+---------+----------+----------+--------------+--------------------+---------------------+----------------------+--------+------------+------------+\n",
      "| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|   939679200|  1008025200|\n",
      "| 10000025|1999-10-13|2017-03-09|     125000.00|                 DKK|                 null|                  null|    6357|   939765600|  1489014000|\n",
      "| 10000122|1999-10-14|2002-08-15|     125000.00|                 DKK|                 null|                  null|    1036|   939852000|  1029362400|\n",
      "| 10000157|1999-11-04|2017-03-09|     125000.00|                 DKK|                 null|                  null|    6335|   941670000|  1489014000|\n",
      "| 10000165|1999-10-13|2006-05-18|     500000.00|                 DKK|                 null|                  null|    2409|   939765600|  1147903200|\n",
      "| 10000211|1999-09-29|2017-03-09|     500000.00|                 DKK|                 null|                  null|    6371|   938556000|  1489014000|\n",
      "| 10000238|1987-08-30|1989-07-05|     300000.00|                 DKK|                 null|                  null|     675|   557272800|   615592800|\n",
      "| 10000254|2009-12-29|2017-03-09|    1100500.00|                 DKK|                 null|                  null|    2627|  1262041200|  1489014000|\n",
      "| 10000254|1999-10-12|2000-08-22|     500000.00|                 DKK|                 null|                  null|     315|   939679200|   966895200|\n",
      "| 10000254|2000-08-23|2009-12-28|    1000000.00|                 DKK|                 null|                  null|    3414|   966981600|  1261954800|\n",
      "| 10000262|1999-10-01|2017-03-09|     144000.00|                 DKK|                 null|                  null|    6369|   938728800|  1489014000|\n",
      "| 10000319|1987-08-30|2001-02-27|     300000.00|                 DKK|                 null|                  null|    4930|   557272800|   983228400|\n",
      "| 10000327|1999-10-15|2009-02-25|     125000.00|                 DKK|                 null|                  null|    3421|   939938400|  1235516400|\n",
      "| 10000351|2000-06-15|2000-06-15|      33576.00|                 EUR|                 null|                  null|       0|   961020000|   961020000|\n",
      "| 10000351|2000-04-05|2000-06-14|      16788.00|                 EUR|                 null|                  null|      70|   954885600|   960933600|\n",
      "| 10000351|1999-10-15|2000-04-04|     125000.00|                 DKK|                 null|                  null|     172|   939938400|   954799200|\n",
      "| 10000416|1999-10-14|2017-03-09|     125000.00|                 DKK|                 null|                  null|    6356|   939852000|  1489014000|\n",
      "| 10000424|1999-10-15|2000-04-04|     125000.00|                 DKK|                 null|                  null|     172|   939938400|   954799200|\n",
      "| 10000424|2000-05-12|2000-05-15|      33576.00|                 EUR|                 null|                  null|       3|   958082400|   958341600|\n",
      "| 10000424|2000-04-05|2000-05-11|      16788.00|                 EUR|                 null|                  null|      36|   954885600|   957996000|\n",
      "+---------+----------+----------+--------------+--------------------+---------------------+----------------------+--------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#virkformDf.show(1)\n",
    "checkCols = [\"kortBeskrivelse\",\"langBeskrivelse\",\"virksomhedsformkode\"]\n",
    "\n",
    "#Consistencycheck is kortBeskrivelse and virksomhedsformkode always mapped the same way\n",
    "#check1 = virkformDf.select(checkCols+[\"cvrNummer\"]).distinct().groupby(*checkCols).count()\n",
    "#check1.orderBy(\"kortBeskrivelse\",\"count\").show(check1.count(),truncate=False)\n",
    "\n",
    "#Second test does any companies go from Aps or A/S to other or vice versa?\n",
    "joinCols = [\"cvrNummer\",\"langBeskrivelse\",\"rank\"]\n",
    "cvrCols = [\"cvrNummer\"]\n",
    "gyldigCol = [\"periode_gyldigFra\"]\n",
    "\n",
    "statusChangeWindow = (Window\n",
    "                      .partitionBy(F.col(*cvrCols))\n",
    "                      .orderBy(F.col(\"periode_gyldigFra\").desc()))\n",
    "\n",
    "mainKapitalDf = (pivotOnText(filtAttributDf.drop(\"vaerditype\").drop(\"sidstOpdateret\"),expectedList=extractedList)\n",
    "                 .withColumnRenamed(existing=\"periode_gyldigFra\",new=\"gyldigFra\")\n",
    "                 .withColumnRenamed(existing=\"periode_gyldigTil\",new=\"gyldigTil\")\n",
    "                 .withColumn(col=F.col(\"gyldigFra\").cast(\"date\"),colName=\"gyldigFra\")\n",
    "                 .withColumn(col=F.col(\"gyldigTil\").cast(\"date\"),colName=\"gyldigTil\")\n",
    "                 .withColumn(col=F.coalesce(F.col(\"gyldigTil\"),F.lit(F.current_date())),colName=\"gyldigTil\")\n",
    "                 .withColumn(col=F.datediff(F.col(\"gyldigTil\"),F.col(\"gyldigFra\")),colName=\"datediff\")\n",
    "                 .withColumn(col=F.unix_timestamp(F.col(\"gyldigFra\")),colName=\"timeStampFra\")\n",
    "                 .withColumn(col=F.unix_timestamp(F.col(\"gyldigTil\")),colName=\"timeStampTil\")\n",
    "                 .filter(F.col(\"sekvensnr\")==0)\n",
    "                 .drop(F.col(\"sekvensnr\"))\n",
    "                 .orderBy(\"cvrNummer\")\n",
    "                )\n",
    "mainKapitalDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joinedDf = yearMonDf.join(mainKapitalDf,(yearMonDf[\"ts\"].between(F.col(\"timeStampFra\"),F.col(\"timeStampTil\"))),\"inner\") \n",
    "joinedDf.write.parquet(path=cvrPath+\"/KaptialDataFrame.parquet\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---------+----------+----------+--------------+--------------------+---------------------+----------------------+--------+\n",
      "| aar|maaned|cvrNummer| gyldigFra| gyldigTil|KAPITAL_vaerdi|KAPITALVALUTA_vaerdi|KAPITALKLASSER_vaerdi|KAPITAL_DELVIST_vaerdi|datediff|\n",
      "+----+------+---------+----------+----------+--------------+--------------------+---------------------+----------------------+--------+\n",
      "|1999|    11| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|1999|    12| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|     1| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|     2| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|     3| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|     4| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|     5| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|     6| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|     7| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|     8| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|     9| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|    10| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|    11| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2000|    12| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2001|     1| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2001|     2| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2001|     3| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2001|     4| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2001|     5| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "|2001|     6| 10000009|1999-10-12|2001-12-11|     125000.00|                 DKK|                 null|                  null|     791|\n",
      "+----+------+---------+----------+----------+--------------+--------------------+---------------------+----------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(joinedDf\n",
    " .drop(\"ts\")\n",
    " .drop(\"timeStampFra\")\n",
    " .drop(\"timeStampTil\")\n",
    " .orderBy(\"cvrNummer\",\"aar\",\"maaned\").show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
