{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ce919b82574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformedDfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogDf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformedDfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#transformedModels = [v.stages[-1].computeCost(transformedDfs[i]) for i,v in enumerate(model)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "transformedDfs = [i.transform(logDf) for i in model]\n",
    "costs = [(i,v.stages[-1].computeCost(transformedDfs[i])) for i,v in enumerate(model)]\n",
    "costs\n",
    "\n",
    "#transformedModels = [v.stages[-1].computeCost(transformedDfs[i]) for i,v in enumerate(model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newParamMap =  ({kmeans.k: 10,kmeans.initMode:\"random\"})\n",
    "newModel = pipeline.fit(logDf,newParamMap)\n",
    "#computedModel = pipeline.fit(logDf)\n",
    "\n",
    "#ide til næste gang beregn beste stuff for alle modeller i pipelinen, derefer tag bedste pipeline ud og byg videre på den."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans = newModel.transform(logDf)\n",
    "trans.groupBy(\"prediction\").count().show() # shows the distribution of companies \n",
    "\n",
    "\n",
    "vec = [Row(cluster=i,center=Vectors.dense(v)) for i,v in enumerate(newModel.stages[-1].clusterCenters())]\n",
    "#print(type(vec))\n",
    "SpDf = sqlContext.createDataFrame(data=vec)\n",
    "#SpDf.show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "featureContributionUdf = F.udf(lambda x,y: (x-y)*(x-y),VectorUDT() )\n",
    "sqrtUdf = F.udf(lambda x,y: float(Vectors.norm(vector=x-y,p=2)),DoubleType())\n",
    "printUdf = F.udf(lambda x: type(x),StringType())\n",
    "toDenseUDf = F.udf(lambda x: Vectors.dense(x.toArray()),VectorUDT())\n",
    "#print(np.sum(vec[0][\"vec\"]))\n",
    "joinedDf = (trans\n",
    "            .join(SpDf,on=(trans[\"prediction\"]==SpDf[\"cluster\"]),how=\"left\")\n",
    "            .withColumn(colName=\"features\",col=toDenseUDf(F.col(\"features\")))\n",
    "            .drop(SpDf[\"cluster\"])\n",
    "            .withColumn(colName=\"contribution\",col=featureContributionUdf(F.col(\"features\"),F.col(\"center\")))\n",
    "            .withColumn(colName=\"distance\",col=sqrtUdf(F.col(\"features\"),F.col(\"center\")))\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_range = widgets.IntSlider()\n",
    "display(int_range)\n",
    "\n",
    "def on_value_change(change):\n",
    "    print(change['new'])\n",
    "\n",
    "int_range.observe(on_value_change, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printTotalAndAvgFeatContribution(df,cluster=0,toPrint=False):\n",
    "    joinedRdd = (df\n",
    "                 .select(\"prediction\",\"contribution\")\n",
    "                 .rdd)\n",
    "    #print(joinedRdd.take(1))\n",
    "    summed = joinedRdd.reduceByKey(add)\n",
    "    normedtotalContribute = summed.map(lambda x: (x[0],x[1])).collectAsMap()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return normedtotalContribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stuff = printTotalAndAvgFeatContribution(joinedDf)\n",
    "\n",
    "centers = [(i,np.log(stuff[i])/np.sum(np.log(stuff[i]))) for i in range(0,10)]\n",
    "cols =joinedDf.columns[5:31]\n",
    "centers\n",
    "\n",
    "clusters = np.array([i[1] for i in centers if i[0] in [6,9,4]  ])\n",
    "transposedCluster = np.log1p(clusters.transpose())\n",
    "N =3\n",
    "\n",
    "import colorsys\n",
    "HSV_tuples = [(x*1.0/len(transposedCluster), 0.5, 0.5) for x in range(len(transposedCluster))]\n",
    "RGB_tuples = list(map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples))\n",
    "\n",
    "ind = np.arange(N)\n",
    "#print(ind)# the x locations for the groups\n",
    "width = 0.5 \n",
    "plots = [plt.bar(ind, transposedCluster[1], width, color='#d62728')]  \n",
    "former = transposedCluster[1]\n",
    "for i,v in enumerate(transposedCluster[1:]):\n",
    "    plots.append(plt.bar(ind, v, width, color=RGB_tuples[i],bottom=former))\n",
    "    former += v\n",
    "plt.ylabel('log Scores')\n",
    "plt.title('Component Contribution for outlier clusters')\n",
    "plt.xticks(ind+0.3, ['C_'+str(i) for i in [6,9,4]])\n",
    "plt.legend([p[0] for p in plots], cols,bbox_to_anchor=(1.05, 1.5),loc=2,borderaxespad=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistanceTransformation(Transformer,HasInputCol,HasOutputCol):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, model=None):\n",
    "        super(DistanceTransformation, self).__init__()\n",
    "        kwargs = self.__init__._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, model=None):\n",
    "        kwargs = self.setParams._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset,model):\n",
    "        \n",
    "        \n",
    "        def computeAndInsertClusterCenter(dataset,centers):\n",
    "            '''\n",
    "            Insert a clusterCenter as column.\n",
    "            '''\n",
    "\n",
    "            distanceUdf = F.udf(lambda x,y: float(np.sqrt(np.sum((x-y)*(x-y)))),DoubleType())\n",
    "\n",
    "            return (dataset\n",
    "                    .join(F.broadcast(centers),on=(dataset[\"prediction\"]==centers[\"cluster\"]),how=\"inner\")\n",
    "                    .withColumn(colName=\"distance\",col=distanceUdf(F.col(\"scaledFeatures\"),F.col(\"center\")))\n",
    "                    .drop(\"cluster\")\n",
    "                    .drop(\"features\")\n",
    "                    .drop(\"v2\")\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(getCenters(0))\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(kmeans.k, [2, 4, 10]) \\\n",
    "    .addGrid(kmeans.initSteps, [3,5,10]) \\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create an unsupervised classification evaluator\n",
    "class ElbowEvaluation(Estimator,ValidatorParams):\n",
    "    '''\n",
    "        doc\n",
    "    '''\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, estimator=None, estimatorParamMaps=None, evaluator=None,\n",
    "                 seed=None):\n",
    "        super(ElbowEvaluation, self).__init__()\n",
    "        kwargs = self.__init__._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "    \n",
    "    @keyword_only\n",
    "    def setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None):\n",
    "        kwargs = self.setParams._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    computeDistanceToCenterUdf = F.udf(lambda x,y: (x-y)*(x-y),VectorUDT())\n",
    "    \n",
    "    \n",
    "    def _fit(self, dataset):\n",
    "        est = self.getOrDefault(self.estimator)\n",
    "        epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "        numModels = len(epm)\n",
    "        eva = self.getOrDefault(self.evaluator)\n",
    "        \n",
    "        for j in range(numModels):\n",
    "            model = est.fit(dataset, epm[j])\n",
    "            model.\n",
    "            \n",
    "            metric = eva.evaluate(model.transform(dataset, epm[j]))\n",
    "            metrics[j] += metric\n",
    "        if eva.isLargerBetter():\n",
    "            bestIndex = np.argmax(metrics)\n",
    "        else:\n",
    "            bestIndex = np.argmin(metrics)\n",
    "        bestModel = est.fit(dataset, epm[bestIndex])\n",
    "        return self._copyValues(TrainValidationSplitModel(bestModel, metrics))\n",
    "    \n",
    "    def copy(self, extra=None):\n",
    "        \"\"\"\n",
    "        Creates a copy of this instance with a randomly generated uid\n",
    "        and some extra params. This copies creates a deep copy of\n",
    "        the embedded paramMap, and copies the embedded and extra parameters over.\n",
    "\n",
    "        :param extra: Extra parameters to copy to the new instance\n",
    "        :return: Copy of this instance\n",
    "        \"\"\"\n",
    "        if extra is None:\n",
    "            extra = dict()\n",
    "        newTVS = Params.copy(self, extra)\n",
    "        if self.isSet(self.estimator):\n",
    "            newTVS.setEstimator(self.getEstimator().copy(extra))\n",
    "        # estimatorParamMaps remain the same\n",
    "        if self.isSet(self.evaluator):\n",
    "            newTVS.setEvaluator(self.getEvaluator().copy(extra))\n",
    "        return newTVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ElbowEvaluationModel(Model, ValidatorParams):\n",
    "    \"\"\"\n",
    "    .. note:: Experimental\n",
    "\n",
    "    Model from train validation split.\n",
    "\n",
    "    .. versionadded:: 2.0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bestModel, validationMetrics=[]):\n",
    "        super(TrainValidationSplitModel, self).__init__()\n",
    "        #: best model from cross validation\n",
    "        self.bestModel = bestModel\n",
    "        #: evaluated validation metrics\n",
    "        self.validationMetrics = validationMetrics\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return self.bestModel.transform(dataset)\n",
    "\n",
    "    def copy(self, extra=None):\n",
    "        \"\"\"\n",
    "        Creates a copy of this instance with a randomly generated uid\n",
    "        and some extra params. This copies the underlying bestModel,\n",
    "        creates a deep copy of the embedded paramMap, and\n",
    "        copies the embedded and extra parameters over.\n",
    "        And, this creates a shallow copy of the validationMetrics.\n",
    "\n",
    "        :param extra: Extra parameters to copy to the new instance\n",
    "        :return: Copy of this instance\n",
    "        \"\"\"\n",
    "        if extra is None:\n",
    "            extra = dict()\n",
    "        bestModel = self.bestModel.copy(extra)\n",
    "        validationMetrics = list(self.validationMetrics)\n",
    "        return TrainValidationSplitModel(bestModel, validationMetrics)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
