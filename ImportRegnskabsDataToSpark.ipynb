{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Jun 13, 2016\n",
    "\n",
    "@author: svanhmic\n",
    "\n",
    "This script creates a compiled csv file with the test csv records. The csv files containing the accounts are converted from a column-based representation to a row based\n",
    "representation. meaning:\n",
    "account alpha\n",
    "var x , val x date x\n",
    "var y , val y date y\n",
    "...\n",
    "\n",
    "to \n",
    "\n",
    "account aplha [var x, val x , date x], beta[var x , val x , date x] \n",
    "\n",
    "'''\n",
    "sc.addPyFile('/home/svanhmic/workspace/Python/Erhvervs/src/RegnSkabData/RegnskabsClass.py') # this adds the class regnskabsClass to the spark execution\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StringType,StructType, ArrayType,IntegerType,DateType\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from RegnskabsClass import Regnskaber\n",
    "import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')\n",
    "\n",
    "#sc = SparkContext(\"local[8]\",\"importRegnskabs\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "folderPath = \"/home/svanhmic/workspace/Python/Erhvervs/data/regnskabsdata/testcsv\"\n",
    "finalXML = \"/home/svanhmic/workspace/Python/Erhvervs/data/regnskabsdata/finalXML\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertToDate(col):\n",
    "    try:\n",
    "        return datetime.strptime(col,'%Y-%M-%d')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFilesForTaxonomy(fileNamesDf,taxTypeDf):\n",
    "    '''\n",
    "    Method: This method compares the csv-files in a folder and checks whether the csv-file has a taxonomy.\n",
    "    \n",
    "    Input:\n",
    "        fileNamesDf - Spark Data frame that contains the files that\n",
    "        taxTypeDf -  Spark Data frame that contains the taxonomys for all csv-files\n",
    "    \n",
    "    Output:\n",
    "        A list of csv-files that contains the taxonomy with most occurences \n",
    "    \n",
    "    '''\n",
    "    minDf = taxTypeDf.select(F.concat(F.regexp_extract(\"file\",'(\\w+)/(\\d+-\\d+-\\d+.xml)',2),F.lit(\".csv\")).alias(\"file\"),taxTypeDf[\"taxonomy\"]).cache()\n",
    "    #minDf.show(5,truncate=False)\n",
    "    \n",
    "    intersectFilesDf = (fileNamesDf\n",
    "                        .join(minDf,minDf[\"file\"] == fileNamesDf[\"file\"],\"inner\")\n",
    "                        .drop(fileNamesDf[\"file\"])) # join list fo files with list of files with taxonomy, so we can single out those records we want to analyze\n",
    "    #intersectFilesDf.show(20,truncate=False)\n",
    "\n",
    "    groupedIntersectFilesDf = intersectFilesDf.groupBy(\"taxonomy\").count()\n",
    "    #groupedIntersectFilesDf.orderBy(groupedIntersectFilesDf[\"count\"].desc()).show(truncate=False) # show the different types of tax'\n",
    "    \n",
    "    mostTaxonomy = groupedIntersectFilesDf.orderBy(groupedIntersectFilesDf[\"count\"].desc()).first()[\"taxonomy\"]\n",
    "    print(mostTaxonomy)\n",
    "    filteredCsvDf = intersectFilesDf.filter(intersectFilesDf[\"taxonomy\"] == mostTaxonomy)\n",
    "    return [str(f[\"file\"]) for f in filteredCsvDf.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    lengthUdf = F.udf(lambda x: len(x), IntegerType()) # user def methods \n",
    "    convertToDateUdf = F.udf(convertToDate,DateType()) # user def methods\n",
    "    \n",
    "    files = os.listdir(folderPath) # gets all the files in csv\n",
    "    fileNamesDf = sqlContext.createDataFrame([Row(file=f) for f in files]) # import of csv files to dataframe   \n",
    "    \n",
    "    struct = StructType().add(\"file\",StringType(),True).add(\"taxonomy\",StringType(),True)\n",
    "    taxTypeDf = sqlContext.read.csv(finalXML+\"/taxlist.csv\",header=False,schema=struct,sep=\";\")\n",
    "    \n",
    "    recordList = extractFilesForTaxonomy(fileNamesDf,taxTypeDf)\n",
    "    #print(recordList)\n",
    "    list = []\n",
    "    for f in recordList:\n",
    "        list.append(Regnskaber(folderPath+\"/\"+f))\n",
    "        \n",
    "    print(\"Done with all file\")\n",
    "    \n",
    "    df = sqlContext.createDataFrame(list)\n",
    "    del(list)\n",
    "    df.printSchema()\n",
    "    df.show()\n",
    "    #print(df.take(1))\n",
    "    listCsvDf = df.drop(\"file\").select(F.explode(F.col(\"field\")))\n",
    "    valueCsvDf = listCsvDf.select(F.regexp_replace(listCsvDf[\"col\"][\"name\"],r\"\\w+:\",\"\").alias(\"name\")\n",
    "                                   ,listCsvDf[\"col\"][\"id\"].cast(\"integer\").alias(\"id\")\n",
    "                                   ,listCsvDf[\"col\"][\"value\"].alias(\"value\")\n",
    "                                   ,listCsvDf[\"col\"][\"unit\"].alias(\"unit\")\n",
    "                                   ,listCsvDf[\"col\"][\"contextRef\"].alias(\"contextRef\")\n",
    "                                   ,listCsvDf[\"col\"][\"startDate\"].alias(\"startDate\")\n",
    "                                   ,listCsvDf[\"col\"][\"endDate\"].alias(\"endDate\"))\n",
    "    \n",
    "    valueCsvDf.show(truncate=False)\n",
    "    #orderedListCsvDf = listCsvDf.orderBy(listCsvDf[\"fieldlength\"].desc()).select(listCsvDf[\"fieldlength\"])\n",
    "    #newDf = df.select(df[\"field\"][\"name\"].alias(\"name\"),df[\"field\"][\"value\"].alias(\"value\"))\n",
    "    valueCsvDf.write.csv(\"/home/svanhmic/workspace/Python/Erhvervs/data/regnskabsdata/sparkdata/csv/regnskabsdata.csv\",mode='overwrite',header=True,sep=\";\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #plt.eventplot(variableOccurance)\n",
    "    #plt.show()\n",
    "    #print(variableOccurance[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
