{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook should contain a pipeline method for bancruptcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "SQLContext.newSession(sqlContext)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, ValidatorParams\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, DoubleType, IntegerType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, Matrix, MatrixUDT, DenseMatrix\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row, Window, functions as F\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark import keyword_only \n",
    "#from spark_sklearn import GridSearchCV,Converter\n",
    "#from sklearn.cluster import KMeans as skKmeans\n",
    "#from sklearn.linear_model import LogisticRegression as skLogistic\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import random\n",
    "from prettytable import PrettyTable\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from operator import add\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from spark_sklearn import GridSearchCV,Converter\n",
    "PATH = \"/home/svanhmic/workspace/Python/Erhvervs/data/cdata/\"\n",
    "sc.addPyFile(\"/home/svanhmic/workspace/Python/Erhvervs/src/cvr/GridSearchLogRegAndKmeans.py\")\n",
    "sc.addPyFile(\"/home/svanhmic/workspace/Python/Erhvervs/src/cvr/ConvertAllToVecToMl.py\")\n",
    "\n",
    "import GridSearchLogRegAndKmeans\n",
    "from ConvertAllToVecToMl import ConvertAllToVecToMl as convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### import the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#RAW DATA!!! \n",
    "df = sqlContext.read.parquet(PATH+\"featureDataCvr\")\n",
    "\n",
    "#exclude some of the variables, and cast all variables to double\n",
    "excludeCols = [\"medArb_\"+str(i) for i in range(1,16)] # we don't need the medarbejders \n",
    "includeCols = [i for i in df.columns if i not in excludeCols]\n",
    "rankCols = [re.sub(pattern=\"rank_\",repl=\"vaerdiSlope_\",string=i) for i in includeCols]\n",
    "finalCols = [F.col(i) for i in includeCols[:2]]+[F.col(i).cast(\"double\") for i in includeCols[2:]]\n",
    "\n",
    "renamedDf = (df\n",
    "             .select(*finalCols)\n",
    "             .select([F.col(val).alias(rankCols[idx]) for idx,val in enumerate(includeCols)])\n",
    "            )\n",
    "\n",
    "\n",
    "#renamedDf.show(3)\n",
    "#renamedDf.printSchema()\n",
    "#import name data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import company names with CVR-number\n",
    "windowSpecRank =(Window.partitionBy(F.col(\"cvrNummer\"))).orderBy(F.col(\"gyldigFra\").desc())\n",
    "groupCols = [\"cvrNummer\",\"vaerdi\"]\n",
    "\n",
    "companyNameDf = (sqlContext\n",
    "                 .read\n",
    "                 .parquet(PATH+\"companyCvrData\")\n",
    "                 .withColumn(colName=\"rank\",col=F.rank().over(windowSpecRank))\n",
    "                 .filter((F.col(\"rank\")==1) & (F.col(\"sekvensnr\")==0))\n",
    "                 .select([F.col(i) for i in groupCols])\n",
    "                 .withColumnRenamed(existing=\"vaerdi\",new=\"navn\")\n",
    "                 .orderBy(F.col(\"cvrNummer\"))\n",
    "                 .cache()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build own transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "labelCols = [\"navn\",\"cvrNummer\",\"label\",\"status\"]\n",
    "featCols = [i for i in companyNameDf.columns+renamedDf.columns if i not in labelCols]\n",
    "\n",
    "#get minimum values from each column\n",
    "minCols = [F.min(i).alias(i) for i in featCols]\n",
    "minValsRdd = renamedDf.groupby().agg(*minCols).rdd\n",
    "broadcastedmin = sc.broadcast(minValsRdd.first().asDict())\n",
    "\n",
    "#create array that subtracts minimum value in the numeric columns.\n",
    "logColsSelected = [F.col(i).alias(i) for i in labelCols]+[(F.col(i)-F.lit(broadcastedmin.value[i])).alias(i) for i in featCols]\n",
    "\n",
    "#takes log(x+1) to the numeric columns and fills the blanks with 0.0 \n",
    "logDf = (renamedDf\n",
    "         .join(companyNameDf,(companyNameDf[\"cvrNummer\"]==renamedDf[\"cvrNummer\"]),\"inner\")\n",
    "         .drop(companyNameDf[\"cvrNummer\"])\n",
    "         .select(*logColsSelected)\n",
    "         .select([F.col(i).alias(i) for i in labelCols]+[F.log1p(F.col(i)).alias(i) for i in featCols])\n",
    "         .distinct()\n",
    "         .na\n",
    "         .fill(0.0,featCols)\n",
    "         .cache()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeAndInsertClusterCenter(dataset,centers):\n",
    "    '''\n",
    "        Insert a clusterCenter as column.\n",
    "    '''\n",
    "    \n",
    "    distanceUdf = F.udf(lambda x,y: float(np.sqrt(np.sum((x-y)*(x-y)))),DoubleType())\n",
    "    \n",
    "    return (dataset\n",
    "            .join(F.broadcast(centers),on=(dataset[\"prediction\"]==centers[\"cluster\"]),how=\"inner\")\n",
    "            .withColumn(colName=\"distance\",col=distanceUdf(F.col(\"scaledFeatures\"),F.col(\"center\")))\n",
    "            .drop(\"cluster\")\n",
    "            .drop(\"features\")\n",
    "            .drop(\"v2\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = VectorAssembler(inputCols=featCols,outputCol=\"features\")\n",
    "con = convert(inputCol=vectorizer.getOutputCol(),outputCol=\"v2\")\n",
    "standardScale = StandardScaler(withMean=True,withStd=True,inputCol=con.getOutputCol(),outputCol=\"scaledFeatures\")\n",
    "kmeans = KMeans(featuresCol=standardScale.getOutputCol(),predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorizer,con,standardScale,kmeans])\n",
    "\n",
    "paramMap = ({kmeans.k: 3,kmeans.initMode:\"random\"}\n",
    "            ,{kmeans.k: 4,kmeans.initMode:\"random\"}\n",
    "            ,{kmeans.k: 5,kmeans.initMode:\"random\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#builld the model for the pipeline\n",
    "model = pipeline.fit(logDf,params=paramMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformedDfs = [i.transform(logDf) for i in model]\n",
    "transformedModels = [v.stages[-1].computeCost(transformedDfs[i]) for i,v in enumerate(model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformedModels\n",
    "bestModel = model[-1]\n",
    "\n",
    "#ide til næste gang beregn beste stuff for alle modeller i pipelinen, derefer tag bedste pipeline ud og byg videre på den."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.Pipeline"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'computeCost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-c7eb5ca73d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#transformed = model.transform(logDf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#rowCenters = [Row(cluster=i,center=Vectors.dense(k)) for i,k in list(enumerate(model.stages[-1].clusterCenters()))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#centersDf = sc.parallelize(rowCenters).toDF()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#transformedCenter = computeAndInsertClusterCenter(transformed,centersDf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-c7eb5ca73d1d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#transformed = model.transform(logDf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#rowCenters = [Row(cluster=i,center=Vectors.dense(k)) for i,k in list(enumerate(model.stages[-1].clusterCenters()))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#centersDf = sc.parallelize(rowCenters).toDF()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#transformedCenter = computeAndInsertClusterCenter(transformed,centersDf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 844\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'computeCost'"
     ]
    }
   ],
   "source": [
    "print([i.computeCost() for i in models])\n",
    "#transformed = model.transform(logDf)\n",
    "#rowCenters = [Row(cluster=i,center=Vectors.dense(k)) for i,k in list(enumerate(model.stages[-1].clusterCenters()))]\n",
    "#centersDf = sc.parallelize(rowCenters).toDF()\n",
    "#transformedCenter = computeAndInsertClusterCenter(transformed,centersDf)\n",
    "#transformedCenter.select(\"cvrNummer\",\"prediction\",\"distance\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans_4423b3fa5e143c7a1fd3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = model.stages[-1]\n",
    "km\n",
    "#km.computeCost(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DistanceTransformation(Transformer,HasInputCol,HasOutputCol):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, model=None):\n",
    "        super(DistanceTransformation, self).__init__()\n",
    "        kwargs = self.__init__._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, model=None):\n",
    "        kwargs = self.setParams._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset,model):\n",
    "        \n",
    "        \n",
    "        def computeAndInsertClusterCenter(dataset,centers):\n",
    "            '''\n",
    "            Insert a clusterCenter as column.\n",
    "            '''\n",
    "\n",
    "            distanceUdf = F.udf(lambda x,y: float(np.sqrt(np.sum((x-y)*(x-y)))),DoubleType())\n",
    "\n",
    "            return (dataset\n",
    "                    .join(F.broadcast(centers),on=(dataset[\"prediction\"]==centers[\"cluster\"]),how=\"inner\")\n",
    "                    .withColumn(colName=\"distance\",col=distanceUdf(F.col(\"scaledFeatures\"),F.col(\"center\")))\n",
    "                    .drop(\"cluster\")\n",
    "                    .drop(\"features\")\n",
    "                    .drop(\"v2\")\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.24316123  1.29036901  1.43017047  1.57093235  1.68765505  1.79981332\n",
      "  1.89225804  1.96319964  2.00468028  2.00085553  1.96622339  1.90925039\n",
      "  1.81417976  1.67450273  1.56527918  1.07171302  1.19343275  1.35238984\n",
      "  1.49336364  1.62851051  1.75304658  1.85680779  1.93851861  1.99426609\n",
      "  2.00190868  1.97999797  1.92952839  1.83842153  1.70008073  1.59322725\n",
      "  0.0058728   0.29533131  0.53410141  0.77398575  0.71721803  0.60442868\n",
      "  0.50440329  0.41001621  0.34144952  0.28307943  0.0791012 ]\n"
     ]
    }
   ],
   "source": [
    "print(getCenters(0))\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(kmeans.k, [2, 4, 10]) \\\n",
    "    .addGrid(kmeans.initSteps, [3,5,10]) \\\n",
    "    .build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create an unsupervised classification evaluator\n",
    "class ElbowEvaluation(Estimator,ValidatorParams):\n",
    "    '''\n",
    "        doc\n",
    "    '''\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, estimator=None, estimatorParamMaps=None, evaluator=None,\n",
    "                 seed=None):\n",
    "        super(ElbowEvaluation, self).__init__()\n",
    "        kwargs = self.__init__._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "    \n",
    "    @keyword_only\n",
    "    def setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None):\n",
    "        kwargs = self.setParams._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    computeDistanceToCenterUdf = F.udf(lambda x,y: (x-y)*(x-y),VectorUDT())\n",
    "    \n",
    "    \n",
    "    def _fit(self, dataset):\n",
    "        est = self.getOrDefault(self.estimator)\n",
    "        epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "        numModels = len(epm)\n",
    "        eva = self.getOrDefault(self.evaluator)\n",
    "        \n",
    "        for j in range(numModels):\n",
    "            model = est.fit(dataset, epm[j])\n",
    "            model.\n",
    "            \n",
    "            metric = eva.evaluate(model.transform(dataset, epm[j]))\n",
    "            metrics[j] += metric\n",
    "        if eva.isLargerBetter():\n",
    "            bestIndex = np.argmax(metrics)\n",
    "        else:\n",
    "            bestIndex = np.argmin(metrics)\n",
    "        bestModel = est.fit(dataset, epm[bestIndex])\n",
    "        return self._copyValues(TrainValidationSplitModel(bestModel, metrics))\n",
    "    \n",
    "    def copy(self, extra=None):\n",
    "        \"\"\"\n",
    "        Creates a copy of this instance with a randomly generated uid\n",
    "        and some extra params. This copies creates a deep copy of\n",
    "        the embedded paramMap, and copies the embedded and extra parameters over.\n",
    "\n",
    "        :param extra: Extra parameters to copy to the new instance\n",
    "        :return: Copy of this instance\n",
    "        \"\"\"\n",
    "        if extra is None:\n",
    "            extra = dict()\n",
    "        newTVS = Params.copy(self, extra)\n",
    "        if self.isSet(self.estimator):\n",
    "            newTVS.setEstimator(self.getEstimator().copy(extra))\n",
    "        # estimatorParamMaps remain the same\n",
    "        if self.isSet(self.evaluator):\n",
    "            newTVS.setEvaluator(self.getEvaluator().copy(extra))\n",
    "        return newTVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ElbowEvaluationModel(Model, ValidatorParams):\n",
    "    \"\"\"\n",
    "    .. note:: Experimental\n",
    "\n",
    "    Model from train validation split.\n",
    "\n",
    "    .. versionadded:: 2.0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bestModel, validationMetrics=[]):\n",
    "        super(TrainValidationSplitModel, self).__init__()\n",
    "        #: best model from cross validation\n",
    "        self.bestModel = bestModel\n",
    "        #: evaluated validation metrics\n",
    "        self.validationMetrics = validationMetrics\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        return self.bestModel.transform(dataset)\n",
    "\n",
    "    def copy(self, extra=None):\n",
    "        \"\"\"\n",
    "        Creates a copy of this instance with a randomly generated uid\n",
    "        and some extra params. This copies the underlying bestModel,\n",
    "        creates a deep copy of the embedded paramMap, and\n",
    "        copies the embedded and extra parameters over.\n",
    "        And, this creates a shallow copy of the validationMetrics.\n",
    "\n",
    "        :param extra: Extra parameters to copy to the new instance\n",
    "        :return: Copy of this instance\n",
    "        \"\"\"\n",
    "        if extra is None:\n",
    "            extra = dict()\n",
    "        bestModel = self.bestModel.copy(extra)\n",
    "        validationMetrics = list(self.validationMetrics)\n",
    "        return TrainValidationSplitModel(bestModel, validationMetrics)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
